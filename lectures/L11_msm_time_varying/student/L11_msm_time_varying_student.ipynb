{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f180ae5",
   "metadata": {},
   "source": [
    "# PHS564 — Lecture 11 (Student)\n",
    "## Time-varying treatment and confounding: marginal structural models (MSMs) (MIMIC-IV Demo)\n",
    "\n",
    "### Learning goals\n",
    "- Recognize time-varying confounding affected by prior treatment.\n",
    "- Construct stabilized weights over time:\n",
    "- treatment weights \\(W_A\\)\n",
    "- censoring weights \\(W_C\\)\n",
    "- combined weights \\(W=W_A\\times W_C\\)\n",
    "- Fit a simple MSM (weighted pooled logistic/GLM) and interpret the parameter as a **marginal causal effect** under assumptions.\n",
    "\n",
    "### Required reading\n",
    "- Hernán & Robins, MSM sections (typically Chapter 12 continuation / longitudinal chapters).\n",
    "\n",
    "**Rules for this notebook**\n",
    "- Only edit cells marked **TODO**.\n",
    "- Do not change the overall structure/cell order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab bootstrap (run this first if you opened from a Colab badge)\n",
    "# - Clones the repo into /content/PHS564 (if needed)\n",
    "# - Installs requirements\n",
    "# - Adds repo to sys.path\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _in_colab() -> bool:\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "\n",
    "if _in_colab():\n",
    "    REPO_URL = \"https://github.com/vafaei-ar/PHS564.git\"\n",
    "    TARGET_DIR = Path(\"/content/PHS564\")\n",
    "\n",
    "    if not (TARGET_DIR / \"requirements.txt\").exists():\n",
    "        print(\"Cloning course repo into Colab runtime...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(TARGET_DIR)], check=True)\n",
    "\n",
    "    os.chdir(TARGET_DIR)\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"-r\", \"requirements.txt\"], check=True)\n",
    "\n",
    "    if str(TARGET_DIR) not in sys.path:\n",
    "        sys.path.insert(0, str(TARGET_DIR))\n",
    "\n",
    "    print(\"✓ Colab setup complete. Now run the rest of the notebook.\")\n",
    "else:\n",
    "    print(\"Not running in Colab; skipping Colab bootstrap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40445ac9",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "This notebook is designed to run **locally** or in **Google Colab**.\n",
    "\n",
    "**If you opened from the Colab badge (recommended):**\n",
    "1) Run the first code cell titled **“Colab bootstrap”** (it clones the repo + installs requirements)\n",
    "2) Run the notebook top-to-bottom.\n",
    "\n",
    "**If you are running locally:**\n",
    "- Install dependencies from `requirements.txt` (see the repo `README.md`), then run top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "RNG = np.random.default_rng(564)\n",
    "\n",
    "# Locate repo root (works when running from lectures/Lxx.../student or /instructor)\n",
    "THIS_DIR = Path.cwd()\n",
    "REPO_ROOT = THIS_DIR\n",
    "for _ in range(4):\n",
    "    if (REPO_ROOT / \"requirements.txt\").exists() or (REPO_ROOT / \"README.md\").exists():\n",
    "        break\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "print(\"Working directory:\", THIS_DIR)\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"Processed data dir exists:\", PROC_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485aac1e",
   "metadata": {},
   "source": [
    "### Build the processed cohort extract (required)\n",
    "\n",
    "This lecture expects an analysis-ready cohort file in `data/processed/`.\n",
    "\n",
    "If it’s missing, run the next cell to:\n",
    "1) download MIMIC-IV Demo into `data/raw/` (if needed)\n",
    "2) build the processed cohort extracts into `data/processed/`\n",
    "\n",
    "**Exposure mode:** the cohort builder uses a simple teaching definition of time-varying treatment derived from vitals. You may change thresholds in the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06375d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build processed cohort extracts from raw MIMIC-IV Demo (safe to re-run)\n",
    "# This will create cohort files under data/processed/.\n",
    "\n",
    "HR_THRESHOLD = 100.0  # defines A_t = 1 if mean daily HR > threshold\n",
    "T_MAX = 7            # number of ICU days to include (teaching simplification)\n",
    "\n",
    "try:\n",
    "    from data.download_data import download_mimic_demo\n",
    "    from data.build_processed_extracts_demo import build_processed_extracts\n",
    "\n",
    "    mimic_dir = RAW_DIR / \"mimic-iv-demo-2.2\"\n",
    "    if not mimic_dir.exists():\n",
    "        print(\"Downloading raw MIMIC-IV Demo (v2.2) to data/raw/ ...\")\n",
    "        download_mimic_demo(out_dir=RAW_DIR, version=\"2.2\", method=\"python\")\n",
    "    else:\n",
    "        print(\"✓ Raw MIMIC-IV Demo already present.\")\n",
    "\n",
    "    out_paths = build_processed_extracts(hr_threshold=HR_THRESHOLD, t_max=T_MAX)\n",
    "    print(\"✓ Built processed cohorts:\")\n",
    "    for k, v in out_paths.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "except Exception as e:\n",
    "    print(\"Could not build processed cohort extracts in this environment.\")\n",
    "    print(\"Error:\", e)\n",
    "    print(\"If you already have the cohort file, place it in data/processed/ and re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2739d9",
   "metadata": {},
   "source": [
    "### Optional: download raw MIMIC-IV Demo tables\n",
    "\n",
    "Not required for the homework pipeline. Skip unless your instructor asks you to explore the raw Demo tables in `data/raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1461bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: explore raw tables (safe to skip)\n",
    "# The cohort-build cell above already downloads what you need for this lecture.\n",
    "\n",
    "try:\n",
    "    from data.download_data import download_mimic_demo\n",
    "\n",
    "    download_mimic_demo(out_dir=RAW_DIR, version=\"2.2\", method=\"python\")\n",
    "except Exception as e:\n",
    "    print(\"Skipping raw MIMIC-IV Demo download.\")\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc24c0e",
   "metadata": {},
   "source": [
    "## Data\n",
    "This lecture uses a **processed cohort extract** derived from MIMIC-IV Demo.\n",
    "\n",
    "Expected file: `data/processed/cohort_L11_msm_longitudinal.parquet` (or `.csv`).\n",
    "\n",
    "If this file is missing:\n",
    "- Run the “Build the processed cohort extract” cell above, or\n",
    "- Run locally: `python data/build_processed_extracts_demo.py --exposure-mode admission_type`\n",
    "\n",
    "Assumed **long** format with columns like:\n",
    "- `stay_id`, `t_day`\n",
    "- treatment `A_t`\n",
    "- time-varying covariates like `hr_mean`\n",
    "- outcome `Y` at end\n",
    "\n",
    "This notebook is **diagnostics-first**: you will compute stabilized weights using a provided scaffold, then **diagnose** and **interpret** the MSM estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528394d",
   "metadata": {},
   "source": [
    "### Column definitions (this cohort)\n",
    "\n",
    "These are defined by the cohort builder (`data/build_processed_extracts_demo.py`):\n",
    "\n",
    "- **Unit + time index**: one row per ICU stay-day\n",
    "  - `stay_id`: ICU stay identifier\n",
    "  - `t_day`: day since ICU intime (0, 1, 2, ...)\n",
    "- **Time-varying covariate**:\n",
    "  - `hr_mean`: mean heart rate during that ICU day (from `chartevents`)\n",
    "- **Time-varying treatment**:\n",
    "  - `A_t = 1` if `hr_mean > HR_THRESHOLD` (a pedagogical stand-in for a time-varying intervention rule)\n",
    "- **Outcome**:\n",
    "  - `Y = 1` if the patient **died in-hospital** (`hospital_expire_flag`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff6d65",
   "metadata": {},
   "source": [
    "### Correction (local build command)\n",
    "\n",
    "The `## Data` cell above may mention an older local build command.\n",
    "\n",
    "For this lecture, the correct local build command is:\n",
    "\n",
    "`python data/build_processed_extracts_demo.py --hr-threshold 100 --t-max 7`\n",
    "\n",
    "(Or, in Colab, just run the build cell at the top of the notebook.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bd8e1",
   "metadata": {},
   "source": [
    "### Before you load the cohort (important)\n",
    "\n",
    "If you **do not** already have `data/processed/cohort_L11_msm_longitudinal.parquet` (or `.csv`):\n",
    "- In Colab: run the cell titled **“Build the processed cohort extract (required)”** above, then come back here.\n",
    "- Locally: run `python data/build_processed_extracts_demo.py --hr-threshold 100 --t-max 7`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe929572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statsmodels for regression (logit/ols); installed via requirements.txt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "parquet_path = PROC_DIR / \"cohort_L11_msm_longitudinal.parquet\"\n",
    "csv_path = PROC_DIR / \"cohort_L11_msm_longitudinal.csv\"\n",
    "\n",
    "if parquet_path.exists():\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "elif csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing cohort file for L11. Build it via:\\n\"\n",
    "        \"  - Colab: run the build cell above\\n\"\n",
    "        \"  - Local: python data/build_processed_extracts_demo.py --hr-threshold 100 --t-max 7\\n\"\n",
    "        \"\\nNote: This notebook focuses on treatment weights (W_A). Censoring weights (W_C) are an extension.\" \n",
    "    )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68ef58",
   "metadata": {},
   "source": [
    "### Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in the built cohort (see data/build_processed_extracts_demo.py)\n",
    "ID = \"stay_id\"\n",
    "T = \"t_day\"\n",
    "A = \"A_t\"  # time-varying treatment\n",
    "Y = \"Y\"    # end-of-follow-up outcome column\n",
    "\n",
    "# Create a simple numeric baseline covariate for modeling\n",
    "if \"sex_male\" not in df.columns:\n",
    "    df[\"sex_male\"] = (df[\"sex\"].astype(str).str.upper() == \"M\").astype(int)\n",
    "\n",
    "# Time-varying covariates available in the extract\n",
    "L_t = [\"hr_mean\"]\n",
    "\n",
    "# Baseline covariates\n",
    "L0 = [\"age\", \"sex_male\"]\n",
    "\n",
    "cols_needed = [ID, T, A, Y] + L0 + L_t\n",
    "missing = [c for c in cols_needed if c not in df.columns]\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee43dac",
   "metadata": {},
   "source": [
    "## Part A — Stabilized treatment weights (scaffold)\n",
    "You should NOT re-derive the formulas here. Focus on model specification and diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilized_treatment_weights(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = data.copy()\n",
    "    d = d.sort_values([ID, T]).reset_index(drop=True)\n",
    "\n",
    "    # Denominator model: Pr(A_t=1 | past A, past L, baseline L0)\n",
    "    # Numerator model:   Pr(A_t=1 | past A, baseline L0)\n",
    "    # NOTE: The exact covariate history depends on your extract. This is a scaffold.\n",
    "\n",
    "    # Create simple lag features (example: lagged treatment and lagged confounder)\n",
    "    d[\"A_lag1\"] = d.groupby(ID)[A].shift(1).fillna(0)\n",
    "    for l in L_t:\n",
    "        d[f\"{l}_lag1\"] = d.groupby(ID)[l].shift(1)\n",
    "\n",
    "    denom_covs = [\"A_lag1\"] + L0 + [f\"{l}_lag1\" for l in L_t]\n",
    "    num_covs   = [\"A_lag1\"] + L0\n",
    "\n",
    "    denom_formula = A + \" ~ \" + \" + \".join([c for c in denom_covs if c in d.columns])\n",
    "    num_formula   = A + \" ~ \" + \" + \".join([c for c in num_covs if c in d.columns])\n",
    "\n",
    "    denom = smf.logit(denom_formula, data=d).fit(disp=False)\n",
    "    num = smf.logit(num_formula, data=d).fit(disp=False)\n",
    "\n",
    "    p_denom = denom.predict(d)\n",
    "    p_num = num.predict(d)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    eps = 1e-6\n",
    "    p_denom = np.clip(p_denom, eps, 1-eps)\n",
    "    p_num = np.clip(p_num, eps, 1-eps)\n",
    "\n",
    "    w_t = np.where(d[A]==1, p_num/p_denom, (1-p_num)/(1-p_denom))\n",
    "    d[\"sw_treat_t\"] = w_t\n",
    "    d[\"sw_treat\"] = d.groupby(ID)[\"sw_treat_t\"].cumprod()\n",
    "    return d\n",
    "\n",
    "d = stabilized_treatment_weights(df)\n",
    "d[[ID,T,A,\"sw_treat_t\",\"sw_treat\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e1904",
   "metadata": {},
   "source": [
    "### TODO A1 — Diagnose weights\n",
    "Plot `sw_treat` and choose a truncation rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5da08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(d[\"sw_treat\"], bins=80)\n",
    "plt.xlabel(\"stabilized treatment weight (cumulative)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"MSM weights before truncation\")\n",
    "plt.show()\n",
    "\n",
    "lo, hi = d[\"sw_treat\"].quantile([0.01,0.99]).to_list()\n",
    "d[\"sw_treat_trunc\"] = d[\"sw_treat\"].clip(lo,hi)\n",
    "(lo,hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e940f",
   "metadata": {},
   "source": [
    "## Part B — Fit an MSM (simple)\n",
    "Example MSM: E[Y] = β0 + β1 * cumulative_treatment.\n",
    "\n",
    "Your extract may define treatment history differently; adapt as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: cumulative treatment exposure up to time t\n",
    "d[\"A_cum\"] = d.groupby(ID)[A].cumsum()\n",
    "\n",
    "# Use the last time point per subject for end-of-follow-up Y\n",
    "last = d.sort_values([ID,T]).groupby(ID).tail(1).copy()\n",
    "\n",
    "# TODO: define the MSM formula\n",
    "msm_formula = Y + \" ~ A_cum\"  # or Y ~ A_t (if outcome is time-specific)\n",
    "\n",
    "msm = smf.ols(msm_formula, data=last).fit(weights=last[\"sw_treat_trunc\"])\n",
    "msm.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a47f3d",
   "metadata": {},
   "source": [
    "### TODO B1 — Compare to naïve (unweighted) estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = smf.ols(msm_formula, data=last).fit()\n",
    "{\"beta1_weighted\": float(msm.params.get(\"A_cum\", np.nan)),\n",
    " \"beta1_naive\": float(naive.params.get(\"A_cum\", np.nan))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83660b7c",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "1) What does the MSM coefficient represent here (marginal vs conditional)?\n",
    "2) Which part is harder: computing weights or interpreting the estimate?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
